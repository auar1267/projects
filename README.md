# Projects
Some of the personal projects I have worked on (at least the ones that were still functional/I still had access to the source code of).

## Geocoded Tweet Distance
A simple program that takes in a dataset consisting of geocoded tweets made around the time of the 2014 Carlton Complex Wildfire, the program takes this information in and based on user input, will display the entire list in an organized fashion or will ask for a distance (in kilometers) and outputs the number of tweets that were within that distance and then displays said tweets. The idea is that a user could extrapolate that the closer in distance that the user specified, the more tweets directly related to the fire would be displayed and test said theory. Coded in C++

##"Smart" Tic Tac Toe
The idea was to build the structure for a game of Tic-Tac-Toe made up of N x N squares (N based on user specification) that would then pit two players against each other, either a "dumb" player who makes random moves, or a "smart" player that uses alpha-beta trees (with pruning) to determine what their best move is. Ideally the "smart" player would never lose against a "dumb" one (the game could still end in a draw, but the "smart" player would never let the game get into a state where the opposing player could put them in a state that the ab player could only potentially lose in). As such if two "smart" players are pitted against each other the game should always end in a draw (and the choices the players make should be more or less the same). Built in a Jupyter Notebook, which uses Python. *Note:* Takes a while to run some of the last bits of code as the "smart" players are examining all potential future states each turn, and with 30 games it takes a little while to finish running.

##Q-learning "Drone"
This program creates a simulated space for a simlated drone (or entity, I just treated it as a drone) where the drone starts in a random position and has to try to land in a target position. Each space has some "reward" associated with it (negative outside of the goal position, positive on the goal), and based on these rewards the drone will start to train using Q-learning as it runs through iterations and learn what direction it should move in based on which position it is in. The ideal situation is that after enough iterations the drone will have been trained enough to fly to the goal regardless of which position it starts in. Also written in a Jupyter Notebook, uses Python. *Note:* Runs thousands of iterations to train, so a takes a little bit of time to run, depending on your machine.# projects